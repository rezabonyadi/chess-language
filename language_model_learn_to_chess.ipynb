{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rezabonyadi/chess-language/blob/main/language_model_learn_to_chess.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KsncjM4SxJ2O"
      },
      "outputs": [],
      "source": [
        "!pip install vllm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iEN1ThNBgbZe"
      },
      "outputs": [],
      "source": [
        "!pip install trl datasets peft python-chess accelerate bitsandbytes unsloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Nmj-aZNCooL"
      },
      "outputs": [],
      "source": [
        "!apt-get install -y stockfish"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "STOCKFISH_PATH = \"/usr/games/stockfish\""
      ],
      "metadata": {
        "id": "5bTWVx0LLE6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QuEolJwHilCp"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import torch\n",
        "from datasets import load_dataset, Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from trl import GRPOConfig, GRPOTrainer\n",
        "\n",
        "# A little trick to help instruct models to do not waste too much time figuring out the required response format.\n",
        "FORMAT_REMINDER = \"\"\"\n",
        " Respond in the following format:\n",
        "<reasoning> your reasoning process here </reasoning>\n",
        "<answer> your final answer here </answer>\n",
        "\"\"\"\n",
        "\n",
        "SYSTEM_PROMPT = f\"\"\"\n",
        "You are going to respond to a user query. You always first reason and then provide your answer.\n",
        "You enclose your reasoning process and answer within <reasoning> </reasoning> and <answer> </answer> tags, respectively, i.e.,\n",
        "{FORMAT_REMINDER}\n",
        " \"\"\"\n",
        "\n",
        "XML_COT_FORMAT = \"\"\"\\\n",
        "<reasoning>\n",
        "{reasoning}\n",
        "</reasoning>\n",
        "<answer>\n",
        "{answer}\n",
        "</answer>\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generating Chess games. Generate games if you dont have them saved, otherwise we will load them later\n",
        "import chess\n",
        "import chess.engine\n",
        "import math\n",
        "import random\n",
        "import chess.svg\n",
        "from IPython.display import SVG\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "from multiprocessing import Pool, cpu_count\n",
        "import json\n",
        "\n",
        "def generate_random_game(min_moves=2, max_moves=10):\n",
        "    board = chess.Board()\n",
        "    moves = []\n",
        "    num_moves = random.randint(min_moves, max_moves)\n",
        "    for _ in range(num_moves):\n",
        "        if board.is_game_over():\n",
        "            break\n",
        "        legal_moves = list(board.legal_moves)\n",
        "        move = random.choice(legal_moves)\n",
        "        board.push(move)\n",
        "        moves.append(move.uci())\n",
        "    return moves, board\n",
        "\n",
        "\n",
        "def generate_realistic_game(min_moves=2, max_moves=10, engine_path=STOCKFISH_PATH):\n",
        "    engine = chess.engine.SimpleEngine.popen_uci(engine_path)\n",
        "\n",
        "    board = chess.Board()\n",
        "    moves = []\n",
        "    num_moves = np.random.randint(min_moves, max_moves)\n",
        "    for _ in range(num_moves):\n",
        "        if board.is_game_over():\n",
        "            break  # Stop if the game is finished\n",
        "\n",
        "        info = engine.analyse(board, chess.engine.Limit(time=0.1), multipv=5)\n",
        "        top_moves = [(tm['pv'][0], tm['score'].relative.score()) for tm in info]\n",
        "\n",
        "        chosen_move = random.choice(top_moves)\n",
        "\n",
        "        board.push(chosen_move[0])\n",
        "        moves.append(str(chosen_move[0]))\n",
        "\n",
        "    engine.close()\n",
        "\n",
        "    return moves, board\n",
        "\n",
        "def generate_puzzle(args):\n",
        "    \"\"\"Function to be executed in parallel.\"\"\"\n",
        "    index, challenging, min_moves, max_moves, stockfish_path = args\n",
        "    engine_path = stockfish_path\n",
        "\n",
        "    moves, board = generate_realistic_game(min_moves, max_moves, engine_path) if challenging else generate_random_game(min_moves, max_moves)\n",
        "\n",
        "    moves_text = \" \".join(moves)\n",
        "    turn = \"white\" if board.turn else \"black\"\n",
        "    prompt_text = (f\"The following chess moves have been played: {moves_text}. \"\n",
        "                   f\"It is {turn} to move. Suggest the best feasible next move for {turn}. {FORMAT_REMINDER}\")\n",
        "\n",
        "    return {\n",
        "        'prompt': [\n",
        "            {'role': 'system', 'content': SYSTEM_PROMPT},\n",
        "            {'role': 'user', 'content': prompt_text}\n",
        "        ],\n",
        "        'boards': board.fen()  # Store board as FEN instead of full object\n",
        "    }\n",
        "\n",
        "def get_chess_puzzles_parallel(num_examples=100, challenging=True, min_moves=2, max_moves=10):\n",
        "    \"\"\"Parallelized chess puzzle generator.\"\"\"\n",
        "    num_workers = min(cpu_count(), 8)  # Use up to 8 cores (adjust if needed)\n",
        "\n",
        "    # Prepare arguments for parallel execution\n",
        "    task_args = [(i, challenging, min_moves, max_moves, STOCKFISH_PATH) for i in range(num_examples)]\n",
        "\n",
        "    # Use multiprocessing Pool\n",
        "    with Pool(num_workers) as pool:\n",
        "        puzzles = list(tqdm(pool.imap(generate_puzzle, task_args), total=num_examples))\n",
        "\n",
        "    return puzzles\n",
        "\n",
        "def save_puzzles(puzzles, filename):\n",
        "    with open(filename, 'w') as file:\n",
        "        json.dump(puzzles, file, indent=4)\n",
        "\n",
        "# Generate chess games. \"challenging=True\" would use stockfish to create games, which takes more time.\n",
        "# False would be just random feasible games.\n",
        "dataset = get_chess_puzzles_parallel(num_examples=50000, challenging=False, min_moves=2, max_moves=6)\n",
        "\n",
        "# Save them for future use, if you want.\n",
        "# save_puzzles(dataset, 'chess_difficult_5_10_moves.json')"
      ],
      "metadata": {
        "id": "MLNghc3OERLT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reward functions for learning Chess\n",
        "import chess\n",
        "import chess.engine\n",
        "import math\n",
        "import random\n",
        "import chess.svg\n",
        "from IPython.display import SVG\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "\n",
        "\n",
        "def parse_chess_move(answer):\n",
        "    answer = answer.replace('.', '')\n",
        "    answer = answer.replace(' ', '')\n",
        "    answer = answer.replace('-', '')\n",
        "    return answer\n",
        "\n",
        "def evaluate_move(board, move_str, turn=None, analysis_time=0.1, scaling=0.003):\n",
        "    max_score = 2.0\n",
        "\n",
        "    valid_move_reward = 0.1\n",
        "    legal_move_reward = 0.1\n",
        "\n",
        "    total_score = 0.0\n",
        "    board = chess.Board(board)\n",
        "    try: # Is it a valid uci move\n",
        "        move = chess.Move.from_uci(move_str)\n",
        "        total_score += valid_move_reward\n",
        "    except:\n",
        "        return total_score\n",
        "\n",
        "    if move in board.legal_moves: # Is it a feasible legal move\n",
        "        total_score += legal_move_reward\n",
        "    else:\n",
        "        return total_score\n",
        "\n",
        "    board_copy = board.copy()\n",
        "    board_copy.push(move)\n",
        "\n",
        "    # Start the engine.\n",
        "    engine = chess.engine.SimpleEngine.popen_uci(STOCKFISH_PATH)\n",
        "    try: # Score, the higher the better\n",
        "        info = engine.analyse(board_copy, chess.engine.Limit(time=analysis_time))\n",
        "        effective_score = -info[\"score\"].relative\n",
        "        # The engine returns a PovScore relative to the opponent (is the move in benefit for opponent).\n",
        "        # We get the negative to be if it is good for the player. The larger the better\n",
        "\n",
        "        # If the score indicates mate, return the extremes.\n",
        "        if effective_score.is_mate():\n",
        "            move_score = (max_score if effective_score.mate() > 0 else 0.0)\n",
        "        else:\n",
        "            # Get the centipawn score (an integer, e.g., +50 means +0.50 pawns advantage)\n",
        "            cp = effective_score.score()\n",
        "            # Map centipawn score to smaller range.\n",
        "            move_score = max_score*(1 + math.tanh(scaling * cp))/2.0\n",
        "\n",
        "        # # Quantize move score to 3 levels, from 0.0 to 0.5*max_score is 0.0, from 0.5*max_score to 0.75*max_score is 1.0,\n",
        "        # # and larger than 0.75*max_score is 2.0. So, do not reward bad moves.\n",
        "        move_score = 0.0 if move_score < 0.5*max_score else 1.0 if move_score < 0.75*max_score else 2.0\n",
        "\n",
        "        total_score += move_score\n",
        "        return total_score\n",
        "    except:\n",
        "        return total_score\n",
        "    finally:\n",
        "        engine.quit()\n",
        "\n",
        "def extract_xml_answer(text: str) -> str:\n",
        "    answer = text.split(\"<answer>\")[-1]\n",
        "    answer = answer.split(\"</answer>\")[0]\n",
        "    return answer.strip()\n",
        "\n",
        "# # Reward functions\n",
        "def correctness_reward_func(prompts, completions, boards, **kwargs) -> list[float]:\n",
        "    responses = [completion[0]['content'] for completion in completions]\n",
        "    q = prompts[0][-1]['content']\n",
        "    extracted_responses = [extract_xml_answer(r) for r in responses]\n",
        "    parsed_moves = [parse_chess_move(e) for e in extracted_responses]\n",
        "    scores = [evaluate_move(board, parsed_move) for board, parsed_move in zip(boards, parsed_moves)]\n",
        "    print('-'*20, f\"\\nQuestion:\\n{q}\", f\"\\nBoard:\\n{chess.Board(boards[0])}\", f\"\\nResponse:\\n{responses[0]}\",\n",
        "          f\"\\nExtracted:\\n{extracted_responses[0]}\", f\"\\nScore:\\n{scores[0]}\")\n",
        "    return scores\n"
      ],
      "metadata": {
        "id": "NLUg5iv88NoW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def load_puzzles(filename):\n",
        "    with open(filename, 'r') as file:\n",
        "        puzzles = json.load(file)\n",
        "    return puzzles\n",
        "\n",
        "# To load the puzzles back into a variable\n",
        "dataset = load_puzzles('chess_difficult_5_10_moves.json')\n",
        "# dataset = get_gsm8k_questions()"
      ],
      "metadata": {
        "id": "ybWfB900q32I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A-kmscACgx6l"
      },
      "outputs": [],
      "source": [
        "# Formatting reward functions\n",
        "def soft_format_reward_func(completions, **kwargs) -> list[float]:\n",
        "    \"\"\"Reward function that checks if the completion has a specific format.\"\"\"\n",
        "    pattern = r\"<reasoning>.*?</reasoning>\\s*<answer>.*?</answer>\"\n",
        "    responses = [completion[0][\"content\"] for completion in completions]\n",
        "    matches = [re.match(pattern, r) for r in responses]\n",
        "    return [0.5 if match else 0.0 for match in matches]\n",
        "\n",
        "def count_xml(text) -> float:\n",
        "    count = 0.0\n",
        "    if text.count(\"<reasoning>\") == 1:\n",
        "        count += 0.125\n",
        "    if text.count(\"</reasoning>\") == 1:\n",
        "        count += 0.125\n",
        "    if text.count(\"<answer>\") == 1:\n",
        "        count += 0.125\n",
        "        count -= len(text.split(\"</answer>\")[-1])*0.001\n",
        "    if text.count(\"</answer>\") == 1:\n",
        "        count += 0.125\n",
        "        count -= (len(text.split(\"</answer>\")[-1]) - 1)*0.001\n",
        "    return count\n",
        "\n",
        "def xmlcount_reward_func(completions, **kwargs) -> list[float]:\n",
        "    contents = [completion[0][\"content\"] for completion in completions]\n",
        "    return [count_xml(c) for c in contents]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load models and train"
      ],
      "metadata": {
        "id": "ulAZgyD21clA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using normal models"
      ],
      "metadata": {
        "id": "oR9COI-L9Dg3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
        "# model_name = \"microsoft/Phi-3.5-mini-instruct\"\n",
        "# model_name = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
        "\n",
        "output_dir=\"outputs/Qwen-0.5B-GRPO\"\n",
        "run_name = f\"{model_name.replace('/', '-')}-Some-ID\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=None\n",
        ").to(\"cuda\")\n",
        "\n",
        "# You can of course use some quantization\n",
        "# model = AutoModelForCausalLM.from_pretrained(model_name,\n",
        "#                                                   device_map=\"auto\",\n",
        "#                                                   load_in_8bit=True,  # Enables 8-bit quantization\n",
        "#                                                   torch_dtype=\"auto\"  # Automatically selects the correct data type\n",
        "#                                                   )\n"
      ],
      "metadata": {
        "id": "2St0PpGe8x2L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We better train the model only partially, as otherwise it needs huge memory and we may change its foundation too much.\n",
        "For Qwen 0.5B, you will be fine with an L4 GPU.\n",
        "\n",
        "Two options here: LORA or select subset of layers to train\n"
      ],
      "metadata": {
        "id": "0cZybpKI9PZ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# LORA\n",
        "# from peft import LoraConfig, get_peft_model\n",
        "\n",
        "# # Define your target modules (for example, you might target the query and value projection layers)\n",
        "# target_modules = [\"q_proj\", \"v_proj\"]  # adjust as needed for your model architecture\n",
        "# target_modules = [\"qkv_proj\", \"o_proj\"]\n",
        "# # Your LoRA configuration\n",
        "# lora_config = LoraConfig(\n",
        "#     r=8,\n",
        "#     lora_alpha=16,\n",
        "#     lora_dropout=0.05,\n",
        "#     bias=\"none\",\n",
        "#     task_type=\"CAUSAL_LM\",\n",
        "#     target_modules=target_modules\n",
        "# )\n",
        "\n",
        "# # ------\n",
        "# # Subset of layers\n",
        "# # Freeze all layers first\n",
        "# for param in model.parameters():\n",
        "#     param.requires_grad = False\n",
        "\n",
        "# last_layers = list(model.named_parameters())  # Adjust based on model specifics\n",
        "# trainable_layers = ['31', '30', '29']\n",
        "# for name, param in last_layers:\n",
        "#     try:\n",
        "#         if name.split('.')[2] in trainable_layers:\n",
        "#             param.requires_grad = True\n",
        "#             print(f\"Unfreezing layer: {name}\")\n",
        "#     except:\n",
        "#         continue\n"
      ],
      "metadata": {
        "id": "rFzOsoW981EO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you want to load a model from a checkpoint\n"
      ],
      "metadata": {
        "id": "TMoSoBhD9TEs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model_weights_path = f\"outputs_1/Qwen-0.5B-GRPO/checkpoint-2500\"\n",
        "# model_weights_path = f\"drive/MyDrive/results/llm_chess/Qwen-0.5B-GRPO-Chess/Qwen-0.5B-GRPO-Chess/checkpoint-2500\"\n",
        "\n",
        "# model = AutoModelForCausalLM.from_pretrained(\n",
        "#     model_weights_path,\n",
        "#     torch_dtype=torch.bfloat16,\n",
        "#     device_map=None\n",
        "# ).to(\"cuda\")\n"
      ],
      "metadata": {
        "id": "PESkbT9J831R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=5e-6)\n",
        "\n",
        "training_args = GRPOConfig(\n",
        "    output_dir=output_dir,\n",
        "    run_name=run_name,\n",
        "    learning_rate=5e-6,\n",
        "    adam_beta1 = 0.9,\n",
        "    adam_beta2 = 0.99,\n",
        "    weight_decay = 0.1,\n",
        "    warmup_ratio = 0.1,\n",
        "    lr_scheduler_type='cosine',\n",
        "    logging_steps=1,\n",
        "    bf16=True,\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=4,\n",
        "    num_generations=8,\n",
        "    max_prompt_length=256,\n",
        "    max_completion_length=200,\n",
        "    num_train_epochs=1,\n",
        "    save_steps=1000,\n",
        "    max_grad_norm=0.1,\n",
        "    log_on_each_node=False,\n",
        "    use_vllm=True,\n",
        "    vllm_gpu_memory_utilization=0.4, #.3,\n",
        "    beta=0.04,\n",
        "    vllm_device=\"cuda:0\",\n",
        "    report_to=\"wandb\" #I'm disabling Wandb.\n",
        ")\n"
      ],
      "metadata": {
        "id": "INwBqj2-87lZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PEFT doesnt seem to work on multi-gpu for this library\n",
        "trainer = GRPOTrainer(\n",
        "    model=model,\n",
        "    processing_class=tokenizer,\n",
        "    optimizers=(optimizer, None),\n",
        "    reward_funcs=[\n",
        "        xmlcount_reward_func,\n",
        "        soft_format_reward_func,\n",
        "        correctness_reward_func],\n",
        "    args=training_args,\n",
        "    train_dataset=dataset,\n",
        "    # peft_config=lora_config,\n",
        ")\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "CSdTMjPP9ATt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using Unsloth"
      ],
      "metadata": {
        "id": "scJEAAoz1ehs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "IF You want to use Unsloth, then use these code below"
      ],
      "metadata": {
        "id": "uIx5knR48jOO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel, PatchFastRL, is_bfloat16_supported\n",
        "PatchFastRL(\"GRPO\", FastLanguageModel)\n",
        "\n",
        "import torch\n",
        "max_seq_length = 512 # Can increase for longer reasoning traces\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"meta-llama/meta-Llama-3.1-8B-Instruct\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    load_in_4bit = True, # False for LoRA 16bit\n",
        "    fast_inference = True, # Enable vLLM fast inference\n",
        "    # max_lora_rank = lora_rank,\n",
        "    gpu_memory_utilization = 0.6, # Reduce if out of memory\n",
        ")\n"
      ],
      "metadata": {
        "id": "YHz6rfkZ1UBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We better train the model only partially, as otherwise it needs huge memory and we may change its foundation too much.\n",
        "\n",
        "lora_rank = 32 # Larger rank = smarter, but slower\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = lora_rank, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
        "    ], # Remove QKVO if out of memory\n",
        "    lora_alpha = lora_rank,\n",
        "    use_gradient_checkpointing = \"unsloth\", # Enable long context finetuning\n",
        "    random_state = 3407,\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "G88rKFAAFEt2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import GRPOConfig, GRPOTrainer\n",
        "training_args = GRPOConfig(\n",
        "    use_vllm = True, # use vLLM for fast inference!\n",
        "    learning_rate = 5e-6,\n",
        "    adam_beta1 = 0.9,\n",
        "    adam_beta2 = 0.99,\n",
        "    weight_decay = 0.1,\n",
        "    warmup_ratio = 0.1,\n",
        "    lr_scheduler_type = \"cosine\",\n",
        "    optim = \"paged_adamw_8bit\",\n",
        "    logging_steps = 1,\n",
        "    bf16 = is_bfloat16_supported(),\n",
        "    fp16 = not is_bfloat16_supported(),\n",
        "    per_device_train_batch_size = 1,\n",
        "    gradient_accumulation_steps = 4, # Increase to 4 for smoother training\n",
        "    num_generations = 6, # Decrease if out of memory\n",
        "    max_prompt_length = 256,\n",
        "    max_completion_length = 200,\n",
        "    # num_train_epochs = 1, # Set to 1 for a full training run\n",
        "    max_steps = 250,\n",
        "    save_steps = 250,\n",
        "    max_grad_norm = 0.1,\n",
        "    report_to = \"wandb\", # Can use none\n",
        "    output_dir = \"outputs\",\n",
        ")"
      ],
      "metadata": {
        "id": "xSWisN3Q4Qv7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use peft at your own risk; not working for me with multi-GPU training\n",
        "trainer = GRPOTrainer(\n",
        "    model=model,\n",
        "    processing_class=tokenizer,\n",
        "    reward_funcs=[\n",
        "        xmlcount_reward_func,\n",
        "        soft_format_reward_func,\n",
        "        correctness_reward_func],\n",
        "    args=training_args,\n",
        "    train_dataset=dataset,\n",
        "    # peft_config=lora_config\n",
        ")\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "bnfQuNPsFzHS"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyMR1pAl+trnt60mwgTKwmts",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}